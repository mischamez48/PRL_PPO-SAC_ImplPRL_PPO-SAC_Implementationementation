{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gym\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that enables to modulate the number of hidden layers\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorStochastic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, action_boundaries, hidden_dim=256, num_hidden_layers=2, device=\"cpu\"):\n",
    "        super(ActorStochastic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "        self.mean = nn.Linear(hidden_dim, a_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, a_dim)\n",
    "\n",
    "        self.action_scale = torch.FloatTensor((action_boundaries.high - action_boundaries.low) / 2.0).to(device)\n",
    "        self.action_bias = torch.FloatTensor((action_boundaries.high + action_boundaries.low) / 2.0).to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        out = self.f(state)\n",
    "        mean = self.mean(out)\n",
    "        log_std = self.log_std(out)\n",
    "        # Clamping the log_std because it is told to be more stable\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        \n",
    "        return mean, log_std\n",
    "\n",
    "    def sampling(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        x_t = dist.rsample() #reparametrization trick implemented by pytorch\n",
    "        squashed = torch.tanh(x_t) #Bounds the action / squashing Gaussian\n",
    "\n",
    "        log_prob = dist.log_prob(x_t)-(2*(np.log(2)-x_t-F.softplus(-2*x_t))).sum(dim=1, keepdim=True)\n",
    "        action = self.action_scale * squashed + self.action_bias\n",
    "        \n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDNN(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(QDNN, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim + a_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        out = self.f(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        # Two DNNs to mitigate positive bias\n",
    "        self.Q1 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "        self.Q2 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q1 = self.Q1(state, action)\n",
    "        q2 = self.Q2(state, action)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC():\n",
    "    def __init__(self, s_dim, a_dim, action_boundaries,hidden_dim_actor=256, hidden_dim_critic=256, \n",
    "                 num_layer_actor=2, num_layer_critic=2, lr_act=3e-4, lr_crit=3e-4, \n",
    "                 gamma=0.99, tau=0.005, alpha=0.2, lambd=0.005, target_upd_inter=1, \n",
    "                 buffer_capacity=int(1000), batch_size=32, grad_steps = 1, device=\"cpu\"):\n",
    "        \n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=buffer_capacity)\n",
    "        self.grad_steps = grad_steps\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.target_upd_inter = target_upd_inter\n",
    "\n",
    "        self.actor = ActorStochastic(s_dim, a_dim, action_boundaries, hidden_dim_actor, num_hidden_layers=num_layer_actor, device=device).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_act)\n",
    "\n",
    "        self.critic = Critic(s_dim, a_dim, hidden_dim_critic, num_hidden_layers=num_layer_critic).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_crit)\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(device)\n",
    "\n",
    "        for param in self.critic_target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def choose_action(self, state):\n",
    "            # Choosing action to give to the environnement and not train the model\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            action, _ = self.actor.sampling(state)\n",
    "\n",
    "        return action.cpu().detach().numpy()[0]\n",
    "    \n",
    "    def critic_train(self, states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sampling(next_states)\n",
    "            q1_next, q2_next = self.critic_target(next_states, next_actions)\n",
    "            min_q_next = torch.min(q1_next, q2_next)\n",
    "            target_q_value = rewards + self.gamma *(torch.ones_like(dones)-dones).unsqueeze(1)*(min_q_next - self.alpha * next_log_probs)\n",
    "            \n",
    "        \n",
    "        #print(target_q_value.shape)\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        #print(q1.shape, q2.shape)    \n",
    "        critic_loss = F.mse_loss(q1, target_q_value) + F.mse_loss(q2, target_q_value)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        return critic_loss\n",
    "\n",
    "    \n",
    "    def actor_train(self, states):\n",
    "        actions, log_probs = self.actor.sampling(states)\n",
    "        q1_actor, q2_actor = self.critic(states, actions)\n",
    "        min_q_actor = torch.min(q1_actor, q2_actor)\n",
    "            \n",
    "        actor_loss = (self.alpha * log_probs - min_q_actor).mean(dim=0)\n",
    "\n",
    "        #print(actor_loss)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return actor_loss\n",
    "\n",
    "    def train(self, update_interval):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None, None\n",
    "\n",
    "        for i in range(self.grad_steps):\n",
    "            \n",
    "            states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "            \n",
    "            # Critic train\n",
    "            self.critic.train()\n",
    "            critic_loss= self.critic_train(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            # Actor train\n",
    "            self.actor.train()\n",
    "            actor_loss = self.actor_train(states)\n",
    "\n",
    "            # Soft update of target networks\n",
    "            if update_interval % self.target_upd_inter == 0:\n",
    "                for target_parameters, parameters in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "                    target_parameters.data.copy_(self.tau * parameters.data + (1.0 - self.tau) * target_parameters.data)\n",
    "\n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "\n",
    "    def add_elements_to_buffer(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def train_SAC(agent, env, n_games, training_interval=1, update_interval=1, print_num_episodes=10):\n",
    "    \n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    warmup = 100\n",
    "\n",
    "    for i in range(n_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        for _ in range(env.spec.max_episode_steps):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            \n",
    "            agent.add_elements_to_buffer(state, action, reward, next_state, done)\n",
    "            state = next_state.squeeze()\n",
    "            update_interval += 1\n",
    "            n_steps += 1\n",
    "            if n_steps % training_interval == 0:\n",
    "                critic_loss, actor_loss = agent.train(update_interval)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-10:])\n",
    "        if i % print_num_episodes == 0:\n",
    "            print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    return score_history\n",
    "\n",
    "def plot_scores(mean_scores, std_scores):\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    episodes = np.arange(1, len(mean_scores) + 1)\n",
    "    ax.plot(episodes, mean_scores, color='green', label='Mean Score')\n",
    "    ax.fill_between(episodes, mean_scores - std_scores, mean_scores + std_scores, color='green', alpha=0.3, label='Std Dev')\n",
    "\n",
    "    ax.set_ylabel('Total reward (= time balanced)', fontsize=20)\n",
    "    ax.set_xlabel('Episode #', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    ax.legend(fontsize=20)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def run_experiments(env, agent_class, agent_kwargs, n_games=300, n_runs=3, training_interval=1, update_interval=1):\n",
    "    all_scores = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        # Reinitialize the agent for each run\n",
    "        agent = agent_class(**agent_kwargs)\n",
    "        env.seed(25)\n",
    "        score_history = train_SAC(agent, env, n_games, training_interval=training_interval, update_interval=update_interval, print_num_episodes=10)\n",
    "        all_scores.append(score_history)\n",
    "    \n",
    "    all_scores = np.array(all_scores)\n",
    "    mean_scores = np.mean(all_scores, axis=0)\n",
    "    std_scores = np.std(all_scores, axis=0)\n",
    "    \n",
    "    return mean_scores, std_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 score -1416.5 avg score -1416.5\n",
      "episode 10 score -1394.9 avg score -1236.0\n",
      "episode 20 score -379.2 avg score -297.3\n",
      "episode 30 score -244.0 avg score -158.4\n",
      "episode 40 score -128.9 avg score -217.3\n",
      "episode 50 score -234.3 avg score -265.3\n",
      "episode 60 score -238.0 avg score -142.9\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "env.seed(25)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "agent_kwargs = {\n",
    "    's_dim': state_dim,\n",
    "    'a_dim': action_dim,\n",
    "    'action_boundaries':env.action_space,\n",
    "    'buffer_capacity': int(1e6),\n",
    "    'hidden_dim_actor': 64,\n",
    "    'hidden_dim_critic': 64,\n",
    "    'num_layer_actor': 3,\n",
    "    'num_layer_critic': 3,\n",
    "    'lr_act': 3e-3,\n",
    "    'lr_crit': 3e-3,\n",
    "    'alpha': 0.2,\n",
    "    'tau': 0.05,\n",
    "    'batch_size': 128,\n",
    "    'grad_steps': 2,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "mean_scores, std_scores = run_experiments(env, SAC, agent_kwargs, n_games=300, n_runs=3, training_interval=3, update_interval=1)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df = pd.DataFrame({'Episode': np.arange(1, len(mean_scores) + 1), 'Mean Score': mean_scores, 'Std Dev': std_scores})\n",
    "df.to_csv('sac_pendulum_good.csv', index=False)\n",
    "\n",
    "# Plot the learning progress\n",
    "plot_scores(mean_scores, std_scores)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain car continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ActorStochastic' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m a_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m agent \u001b[38;5;241m=\u001b[39m SAC(s_dim, a_dim, env\u001b[38;5;241m.\u001b[39maction_space,  buffer_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e6\u001b[39m), hidden_dim_actor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, hidden_dim_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m      7\u001b[0m             num_layer_actor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layer_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, lr_act\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4e-4\u001b[39m, lr_crit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4e-4\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m, grad_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m mean_scores, std_scores \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Save the results to a CSV file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(mean_scores) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Score\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_scores, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStd Dev\u001b[39m\u001b[38;5;124m'\u001b[39m: std_scores})\n",
      "Cell \u001b[1;32mIn[9], line 60\u001b[0m, in \u001b[0;36mrun_experiments\u001b[1;34m(env, agent, n_games, n_runs, training_interval, update_interval)\u001b[0m\n\u001b[0;32m     58\u001b[0m     env\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m25\u001b[39m)\n\u001b[0;32m     59\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m25\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     score_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_SAC\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_num_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     all_scores\u001b[38;5;241m.\u001b[39mappend(score_history)\n\u001b[0;32m     63\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_scores)\n",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m, in \u001b[0;36mtrain_SAC\u001b[1;34m(agent, env, n_games, training_interval, update_interval, print_num_episodes)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# while True:\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mmax_episode_steps):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# if warmup<n_steps:\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# else :\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#     action = env.action_space.sample()\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m, in \u001b[0;36mSAC.choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     33\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[12], line 39\u001b[0m, in \u001b[0;36mActorStochastic.sampling\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     36\u001b[0m dist \u001b[38;5;241m=\u001b[39m Normal(mean, std)\n\u001b[0;32m     37\u001b[0m x_t \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mrsample() \u001b[38;5;66;03m#reparametrization trick implemented by pytorch\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(mean)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Reparameterization trick\u001b[39;00m\n\u001b[0;32m     42\u001b[0m x_t \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m+\u001b[39m std \u001b[38;5;241m*\u001b[39m epsilon\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ActorStochastic' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(25)\n",
    "\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "agent = SAC(s_dim, a_dim, env.action_space,  buffer_capacity=int(1e6), hidden_dim_actor = 64, hidden_dim_critic=64, \n",
    "            num_layer_actor=4, num_layer_critic=4, lr_act=4e-4, lr_crit=4e-4, alpha=0.2, tau=0.005, grad_steps=1, batch_size=64, device=\"cuda\")\n",
    "\n",
    "mean_scores, std_scores = run_experiments(env, agent, n_games=300, n_runs=3, training_interval=10)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df = pd.DataFrame({'Episode': np.arange(1, len(mean_scores) + 1), 'Mean Score': mean_scores, 'Std Dev': std_scores})\n",
    "df.to_csv('sac_mountain.csv', index=False)\n",
    "\n",
    "# Plot the learning progress\n",
    "plot_scores(mean_scores, std_scores)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
