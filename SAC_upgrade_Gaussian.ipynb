{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorStochastic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, action_boundaries, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(ActorStochastic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "        self.mean = nn.Linear(hidden_dim, a_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, a_dim)\n",
    "\n",
    "        self.action_boundaries = action_boundaries\n",
    "\n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        out = self.f(state)\n",
    "        mean = self.mean(out)\n",
    "        log_std = self.log_std(out)\n",
    "\n",
    "        #out = torch.tanh(out)\n",
    "        #log_std = torch.tanh(log_std)\n",
    "        #std = torch.exp(log_std)\n",
    "        #noise = torch.normal(0, 1, out.shape[0])\n",
    "        #action = mean + noise * std\n",
    "        #dist=torch.distributions.Normal(mean, log_std)\n",
    "        # log_std = torch.clamp(log_std, -20, 2)\n",
    "        \n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        \n",
    "        return mean, log_std\n",
    "\n",
    "    def sampling(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        x_t = dist.rsample() #reparametrization trick implemented by pytorch\n",
    "        action = torch.tanh(x_t) #Bounds the action\n",
    "        log_prob = dist.log_prob(x_t) # Log probability(/ies if state in batch)\n",
    "        log_Jacobian = torch.log(1-action**2+1e-10)#.sum(dim=1, keepdim=True)\n",
    "        #print(log_Jacobian)\n",
    "        log_prob = (log_prob-log_Jacobian).sum(dim=1, keepdim=True)\n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDNN(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(QDNN, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim + a_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        out = self.f(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        # Two DNNs to mitigate positive bias\n",
    "        self.Q1 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "        self.Q2 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q1 = self.Q1(state, action)\n",
    "        q2 = self.Q2(state, action)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC():\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim_actor=256, hidden_dim_critic=256, \n",
    "                 num_layer_actor=2, num_layer_critic=2, lr_act=3e-4, lr_crit=3e-4, \n",
    "                 gamma=0.99, tau=0.005, alpha=0.2, lambd=0.005, target_upd_inter=1, \n",
    "                 buffer_capacity=int(1000), batch_size=32, grad_steps = 1, device=\"cpu\"):\n",
    "        \n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=buffer_capacity)\n",
    "        self.grad_steps = grad_steps\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.target_upd_inter = target_upd_inter\n",
    "\n",
    "        self.actor = ActorStochastic(s_dim, a_dim, hidden_dim_actor, num_hidden_layers=num_layer_actor).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_act)\n",
    "\n",
    "        self.critic = Critic(s_dim, a_dim, hidden_dim_critic, num_hidden_layers=num_layer_critic).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_crit)\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(device)\n",
    "\n",
    "    def choose_action(self, state, evaluate=False):\n",
    "        #print(state.shape)\n",
    "        if evaluate:\n",
    "            # Choosing action to give to the environnement and not train the model\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                action, log_prob = self.actor.sampling(state)\n",
    "\n",
    "            return action.cpu().detach().numpy()[0], log_prob\n",
    "        \n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        # To train actor model and critic\n",
    "        action, log_prob = self.actor.sampling(state)\n",
    "        \n",
    "        return action, log_prob\n",
    "    \n",
    "    def critic_train(self, states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.choose_action(next_states, evaluate=False)\n",
    "            q1_next, q2_next = self.critic_target(next_states, next_actions)\n",
    "            min_q_next = torch.min(q1_next, q2_next)\n",
    "            target_q_value = rewards + self.gamma *(torch.ones_like(dones)-dones).unsqueeze(1)*(min_q_next - self.alpha * next_log_probs)\n",
    "            \n",
    "        \n",
    "        #print(target_q_value.shape)\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        #print(q1.shape, q2.shape)    \n",
    "        critic_loss = F.mse_loss(q1, target_q_value) + F.mse_loss(q2, target_q_value)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        return critic_loss\n",
    "\n",
    "    \n",
    "    def actor_train(self, states):\n",
    "        actions, log_probs = self.choose_action(states, evaluate=False)\n",
    "        q1_actor, q2_actor = self.critic(states, actions)\n",
    "        min_q_actor = torch.min(q1_actor, q2_actor)\n",
    "\n",
    "        # print(actions.shape, log_probs.shape)\n",
    "        # print(min_q_actor.shape)\n",
    "        #print((self.alpha * log_probs - min_q_actor).shape)\n",
    "            \n",
    "        actor_loss = (self.alpha * log_probs - min_q_actor).mean(dim=0)\n",
    "\n",
    "        #print(actor_loss)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return actor_loss\n",
    "\n",
    "    def train(self, update_interval):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None, None\n",
    "\n",
    "        for i in range(self.grad_steps):\n",
    "            \n",
    "            states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "            \n",
    "            # Critic train\n",
    "            self.critic.train()\n",
    "            critic_loss= self.critic_train(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            # Actor train\n",
    "            self.actor.train()\n",
    "            actor_loss = self.actor_train(states)\n",
    "\n",
    "            # Soft update of target networks\n",
    "            if update_interval % self.target_upd_inter == 0:\n",
    "                for target_parameters, parameters in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "                    target_parameters.data.copy_(self.tau * parameters.data + (1.0 - self.tau) * target_parameters.data)\n",
    "\n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "\n",
    "    def add_elements_to_buffer(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain car continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\guill\\AppData\\Local\\Temp\\ipykernel_22472\\1723473173.py:110: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
      "C:\\Users\\guill\\AppData\\Local\\Temp\\ipykernel_22472\\1723473173.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic loss:  0.0009523602784611285 actor loss :  -2.334667682647705\n",
      "critic loss:  0.0016016880981624126 actor loss :  -3.056730270385742\n",
      "critic loss:  0.00045496877282857895 actor loss :  -3.138033390045166\n",
      "critic loss:  0.0012823720462620258 actor loss :  -3.108534812927246\n",
      "critic loss:  0.0008015856146812439 actor loss :  -3.1353752613067627\n",
      "critic loss:  0.002074391581118107 actor loss :  -3.020723342895508\n",
      "critic loss:  0.0005456255748867989 actor loss :  -3.178065776824951\n",
      "critic loss:  0.0006264892290346324 actor loss :  -3.126984119415283\n",
      "critic loss:  0.0011450889287516475 actor loss :  -3.2426505088806152\n",
      "critic loss:  0.0005965380114503205 actor loss :  -3.1435346603393555\n",
      "Episode 10, Reward: -25.44703913414886\n",
      "critic loss:  0.0011127421166747808 actor loss :  -3.2255096435546875\n",
      "critic loss:  0.005227414891123772 actor loss :  -3.1086409091949463\n",
      "critic loss:  0.00041657191468402743 actor loss :  -3.038107395172119\n",
      "critic loss:  0.00251865410245955 actor loss :  -2.972572088241577\n",
      "critic loss:  0.0006247491110116243 actor loss :  -3.054497718811035\n",
      "critic loss:  0.0004980508238077164 actor loss :  -3.02093768119812\n",
      "critic loss:  0.004215517081320286 actor loss :  -3.010441303253174\n",
      "critic loss:  0.003911043982952833 actor loss :  -3.113544225692749\n",
      "critic loss:  0.0008294081781059504 actor loss :  -3.0765340328216553\n",
      "critic loss:  0.005126120522618294 actor loss :  -3.000908374786377\n",
      "Episode 20, Reward: -26.457997615609813\n",
      "critic loss:  0.0013149655424058437 actor loss :  -2.959019184112549\n",
      "critic loss:  0.001030518440529704 actor loss :  -2.994734048843384\n",
      "critic loss:  0.0010816508438438177 actor loss :  -3.0419836044311523\n",
      "critic loss:  0.0007383329793810844 actor loss :  -3.1347289085388184\n",
      "critic loss:  0.0008012161124497652 actor loss :  -3.0805466175079346\n",
      "critic loss:  0.0010376523714512587 actor loss :  -3.065031051635742\n",
      "critic loss:  0.0006585530936717987 actor loss :  -3.083649158477783\n",
      "critic loss:  0.000929699745029211 actor loss :  -3.1098549365997314\n",
      "critic loss:  0.0004306663468014449 actor loss :  -3.049794912338257\n",
      "critic loss:  0.0009751644684001803 actor loss :  -2.908008337020874\n",
      "Episode 30, Reward: -26.926800412721636\n",
      "critic loss:  0.000768026860896498 actor loss :  -3.1325302124023438\n",
      "critic loss:  0.0005954240332357585 actor loss :  -3.1066012382507324\n",
      "critic loss:  0.0005788154667243361 actor loss :  -2.9830355644226074\n",
      "critic loss:  0.0015268555143848062 actor loss :  -2.9661736488342285\n",
      "critic loss:  0.0025407453067600727 actor loss :  -3.004826068878174\n",
      "critic loss:  0.00026414997410029173 actor loss :  -3.068310260772705\n",
      "critic loss:  0.0005837338394485414 actor loss :  -3.097355365753174\n",
      "critic loss:  0.0048530129715800285 actor loss :  -3.0971486568450928\n",
      "critic loss:  0.0011213684920221567 actor loss :  -2.99219012260437\n",
      "critic loss:  0.0007723322487436235 actor loss :  -3.1302080154418945\n",
      "Episode 40, Reward: -27.088255361898042\n",
      "critic loss:  0.0008669524686411023 actor loss :  -3.0662426948547363\n",
      "critic loss:  0.0007665262091904879 actor loss :  -3.0396385192871094\n",
      "critic loss:  0.001590277417562902 actor loss :  -2.9487528800964355\n",
      "critic loss:  0.000740278628654778 actor loss :  -2.9417457580566406\n",
      "critic loss:  0.001872402266599238 actor loss :  -2.851489305496216\n",
      "critic loss:  0.0008807425037957728 actor loss :  -3.002315044403076\n",
      "critic loss:  0.001343319658190012 actor loss :  -3.0571389198303223\n",
      "critic loss:  0.0005398568464443088 actor loss :  -2.992377758026123\n",
      "critic loss:  0.0005301649798639119 actor loss :  -2.9591054916381836\n",
      "critic loss:  0.000542582361958921 actor loss :  -2.9851036071777344\n",
      "Episode 50, Reward: -27.505739702938726\n",
      "critic loss:  0.001027110149152577 actor loss :  -3.0718929767608643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m update_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m     critic_loss, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 91\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, update_interval)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Critic train\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 91\u001b[0m critic_loss\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Actor train\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mSAC.critic_train\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     46\u001b[0m     next_actions, next_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(next_states, evaluate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 47\u001b[0m     q1_next, q2_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     min_q_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(q1_next, q2_next)\n\u001b[0;32m     49\u001b[0m     target_q_value \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m(torch\u001b[38;5;241m.\u001b[39mones_like(dones)\u001b[38;5;241m-\u001b[39mdones)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(min_q_next \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m next_log_probs)\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[1;32m---> 13\u001b[0m     q1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ2(state, action)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q1, q2\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mQDNN.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state, action], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m, in \u001b[0;36mLinearLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 6\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "hidden_dim_actor = 64\n",
    "hidden_dim_critic = 64\n",
    "num_layer_actor = 2\n",
    "num_layer_critic = 2\n",
    "lr_act = 3e-4\n",
    "lr_crit = 3e-4\n",
    "gamma = 0.99\n",
    "tau = 0.1\n",
    "alpha = 0.1\n",
    "batch_size = 16\n",
    "num_episodes = 100\n",
    "update_interval = 2\n",
    "target_upd_inter = 1\n",
    "buffer_capacity = int(1e6)\n",
    "grad_step=2\n",
    "\n",
    "#print(a_dim)\n",
    "# Initialize SAC agent\n",
    "agent = SAC(s_dim, a_dim, hidden_dim_actor=hidden_dim_actor, hidden_dim_critic=hidden_dim_critic, \n",
    "            num_layer_actor=num_layer_actor, num_layer_critic=num_layer_critic, lr_act=lr_act, \n",
    "            lr_crit=lr_crit, gamma=gamma, tau=tau, alpha=alpha, batch_size=batch_size, \n",
    "            grad_steps=grad_step, device=device, buffer_capacity=buffer_capacity)\n",
    "\n",
    "# Training loop\n",
    "episode_rewards = []\n",
    "critic_loss=None\n",
    "actor_loss=None\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() # shape of (2,)\n",
    "    episode_reward = 0\n",
    "    for t in range(1000):\n",
    "        action, _ = agent.choose_action(state, evaluate=True)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.add_elements_to_buffer(state, action, reward, next_state, done)\n",
    "        state = next_state.copy()\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if t % update_interval == 0:\n",
    "            critic_loss, actor_loss = agent.train(t)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    if critic_loss is not None:\n",
    "        print(\"critic loss: \",critic_loss, \"actor loss : \", actor_loss)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
    "\n",
    "# Plotting\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('SAC on MountainCarContinuous-v0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Linear Layer\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# Define Actor\n",
    "class ActorStochastic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(ActorStochastic, self).__init__()\n",
    "        layers = [LinearLayer(s_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.f = nn.Sequential(*layers)\n",
    "        self.mean = nn.Linear(hidden_dim, a_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, a_dim)\n",
    "\n",
    "        self.action_scale = torch.FloatTensor((env.action_space.high - env.action_space.low) / 2.0)\n",
    "        self.action_bias = torch.FloatTensor((env.action_space.high + env.action_space.low) / 2.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "        out = self.f(state)\n",
    "        mean = self.mean(out)\n",
    "        log_std = self.log_std(out)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        x_t = dist.rsample()  # Reparameterization trick\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = dist.log_prob(x_t)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "# Define Q Network\n",
    "class QDNN(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(QDNN, self).__init__()\n",
    "        layers = [LinearLayer(s_dim + a_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "        self.f = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        return self.f(x)\n",
    "\n",
    "# Define Critic\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.Q1 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "        self.Q2 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q1 = self.Q1(state, action)\n",
    "        q2 = self.Q2(state, action)\n",
    "        return q1, q2\n",
    "\n",
    "# Define Custom Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, s_dim, a_dim, buffer_capacity=1000000):\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.size = 0\n",
    "        self.ptr = 0\n",
    "\n",
    "        self.states = np.zeros((buffer_capacity, s_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((buffer_capacity, a_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((buffer_capacity, 1), dtype=np.float32)\n",
    "        self.next_states = np.zeros((buffer_capacity, s_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((buffer_capacity, 1), dtype=np.float32)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.buffer_capacity\n",
    "        if self.size < self.buffer_capacity:\n",
    "            self.size += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        states = torch.FloatTensor(self.states[idxs])\n",
    "        actions = torch.FloatTensor(self.actions[idxs])\n",
    "        rewards = torch.FloatTensor(self.rewards[idxs])\n",
    "        next_states = torch.FloatTensor(self.next_states[idxs])\n",
    "        dones = torch.FloatTensor(self.dones[idxs])\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "# Define SAC\n",
    "class SAC:\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim_actor=256, hidden_dim_critic=256, \n",
    "                 num_layer_actor=2, num_layer_critic=2, lr_act=3e-4, lr_crit=3e-4, \n",
    "                 gamma=0.99, tau=0.005, alpha=0.2, buffer_capacity=1000000, batch_size=256, device=\"cpu\"):\n",
    "        \n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.actor = ActorStochastic(s_dim, a_dim, hidden_dim_actor, num_hidden_layers=num_layer_actor).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_act)\n",
    "\n",
    "        self.critic = Critic(s_dim, a_dim, hidden_dim_critic, num_hidden_layers=num_layer_critic).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_crit)\n",
    "\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(device)\n",
    "        for p in self.critic_target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(s_dim, a_dim, buffer_capacity)\n",
    "\n",
    "    def choose_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        if evaluate:\n",
    "            with torch.no_grad():\n",
    "                action, _ = self.actor.sample(state)\n",
    "        else:\n",
    "            action, _ = self.actor.sample(state)\n",
    "        return action.cpu().detach().numpy()[0]\n",
    "\n",
    "    def add_to_buffer(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def update_parameters(self, update_interval):\n",
    "        if self.replay_buffer.size < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = states.to(self.device), actions.to(self.device), rewards.to(self.device), next_states.to(self.device), dones.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample(next_states)\n",
    "            q1_next, q2_next = self.critic_target(next_states, next_actions)\n",
    "            min_q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs\n",
    "            q_target = rewards + (1 - dones) * self.gamma * min_q_next\n",
    "\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        critic_loss = 0.5*F.mse_loss(q1, q_target) + 0.5*F.mse_loss(q2, q_target)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        actions_pred, log_probs = self.actor.sample(states)\n",
    "        q1_actor, q2_actor = self.critic(states, actions_pred)\n",
    "        min_q_actor = torch.min(q1_actor, q2_actor)\n",
    "        actor_loss = (self.alpha * log_probs - min_q_actor).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "        return critic_loss.item(), actor_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5, Reward: -26.592520982310738\n",
      "Episode 10, Reward: -26.236385659628567\n",
      "Episode 15, Reward: -27.542241849307754\n",
      "Episode 20, Reward: -25.199084541555255\n",
      "Episode 25, Reward: -25.54427986853094\n",
      "Episode 30, Reward: -25.925458530710102\n",
      "Episode 35, Reward: -26.996473362267658\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m update_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[60], line 182\u001b[0m, in \u001b[0;36mSAC.update_parameters\u001b[1;34m(self, update_interval)\u001b[0m\n\u001b[0;32m    179\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m log_probs \u001b[38;5;241m-\u001b[39m min_q_actor)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 182\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_param, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparameters()):\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "hidden_dim_actor = 128\n",
    "hidden_dim_critic = 128\n",
    "num_layer_actor = 2\n",
    "num_layer_critic = 2\n",
    "lr_act = 3e-4\n",
    "lr_crit = 3e-5\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "alpha = 0.1\n",
    "batch_size = 64\n",
    "num_episodes = 100\n",
    "update_interval = 1\n",
    "target_upd_inter = 1\n",
    "buffer_capacity = 1000000\n",
    "\n",
    "# Initialize SAC agent\n",
    "agent = SAC(s_dim, a_dim, hidden_dim_actor, hidden_dim_critic, num_layer_actor, num_layer_critic, \n",
    "            lr_act, lr_crit, gamma, tau, alpha, buffer_capacity, batch_size, device)\n",
    "\n",
    "# Training loop\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for t in range(999):\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.add_to_buffer(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if t % update_interval == 0:\n",
    "            agent.update_parameters(update_interval)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    if (episode + 1) % 5 == 0:\n",
    "        print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
    "\n",
    "# Plotting\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('SAC on MountainCarContinuous-v0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
