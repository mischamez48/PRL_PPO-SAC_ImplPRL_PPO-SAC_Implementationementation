{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gym\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorStochastic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, action_boundaries, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(ActorStochastic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "        self.mean = nn.Linear(hidden_dim, a_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, a_dim)\n",
    "\n",
    "        self.action_boundaries = action_boundaries\n",
    "\n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        out = self.f(state)\n",
    "        mean = self.mean(out)\n",
    "        log_std = self.log_std(out)\n",
    "        \n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        \n",
    "        return mean, log_std\n",
    "\n",
    "    def sampling(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        x_t = dist.rsample() #reparametrization trick implemented by pytorch\n",
    "        action = torch.tanh(x_t) #Bounds the action\n",
    "        log_prob = dist.log_prob(x_t) # Log probability(/ies if state in batch)\n",
    "        log_Jacobian = torch.log(1-action**2+1e-10)#.sum(dim=1, keepdim=True)\n",
    "        #print(log_Jacobian)\n",
    "        log_prob = (log_prob-log_Jacobian).sum(dim=1, keepdim=True)\n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDNN(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(QDNN, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim + a_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        out = self.f(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        # Two DNNs to mitigate positive bias\n",
    "        self.Q1 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "        self.Q2 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q1 = self.Q1(state, action)\n",
    "        q2 = self.Q2(state, action)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC():\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim_actor=256, hidden_dim_critic=256, \n",
    "                 num_layer_actor=2, num_layer_critic=2, lr_act=3e-4, lr_crit=3e-4, \n",
    "                 gamma=0.99, tau=0.005, alpha=0.2, lambd=0.005, target_upd_inter=1, \n",
    "                 buffer_capacity=int(1000), batch_size=32, grad_steps = 1, device=\"cpu\"):\n",
    "        \n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=buffer_capacity)\n",
    "        self.grad_steps = grad_steps\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.target_upd_inter = target_upd_inter\n",
    "\n",
    "        self.actor = ActorStochastic(s_dim, a_dim, hidden_dim_actor, num_hidden_layers=num_layer_actor).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_act)\n",
    "\n",
    "        self.critic = Critic(s_dim, a_dim, hidden_dim_critic, num_hidden_layers=num_layer_critic).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_crit)\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(device)\n",
    "\n",
    "    def choose_action(self, state, evaluate=False):\n",
    "        #print(state.shape)\n",
    "        if evaluate:\n",
    "            # Choosing action to give to the environnement and not train the model\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                action, log_prob = self.actor.sampling(state)\n",
    "\n",
    "            return action.cpu().detach().numpy()[0], log_prob\n",
    "        \n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        # To train actor model and critic\n",
    "        action, log_prob = self.actor.sampling(state)\n",
    "        \n",
    "        return action, log_prob\n",
    "    \n",
    "    def critic_train(self, states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.choose_action(next_states, evaluate=False)\n",
    "            q1_next, q2_next = self.critic_target(next_states, next_actions)\n",
    "            min_q_next = torch.min(q1_next, q2_next)\n",
    "            target_q_value = rewards + self.gamma *(torch.ones_like(dones)-dones).unsqueeze(1)*(min_q_next - self.alpha * next_log_probs)\n",
    "            \n",
    "        \n",
    "        #print(target_q_value.shape)\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        #print(q1.shape, q2.shape)    \n",
    "        critic_loss = F.mse_loss(q1, target_q_value) + F.mse_loss(q2, target_q_value)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        return critic_loss\n",
    "\n",
    "    \n",
    "    def actor_train(self, states):\n",
    "        actions, log_probs = self.choose_action(states, evaluate=False)\n",
    "        q1_actor, q2_actor = self.critic(states, actions)\n",
    "        min_q_actor = torch.min(q1_actor, q2_actor)\n",
    "            \n",
    "        actor_loss = (self.alpha * log_probs - min_q_actor).mean(dim=0)\n",
    "\n",
    "        #print(actor_loss)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return actor_loss\n",
    "\n",
    "    def train(self, update_interval):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None, None\n",
    "\n",
    "        for i in range(self.grad_steps):\n",
    "            \n",
    "            states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "            \n",
    "            # Critic train\n",
    "            self.critic.train()\n",
    "            critic_loss= self.critic_train(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            # Actor train\n",
    "            self.actor.train()\n",
    "            actor_loss = self.actor_train(states)\n",
    "\n",
    "            # Soft update of target networks\n",
    "            if update_interval % self.target_upd_inter == 0:\n",
    "                for target_parameters, parameters in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "                    target_parameters.data.copy_(self.tau * parameters.data + (1.0 - self.tau) * target_parameters.data)\n",
    "\n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "\n",
    "    def add_elements_to_buffer(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain car continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Linear Layer\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# Define Actor\n",
    "class ActorStochastic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, action_space, hidden_dim=256, num_hidden_layers=2, device=\"cpu\"):\n",
    "        super(ActorStochastic, self).__init__()\n",
    "        layers = [LinearLayer(s_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.f = nn.Sequential(*layers)\n",
    "        \n",
    "        self.mean = nn.Linear(hidden_dim, a_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, a_dim)\n",
    "\n",
    "        self.action_scale = torch.FloatTensor((action_space.high - action_space.low) / 2.0).to(device)\n",
    "        self.action_bias = torch.FloatTensor((action_space.high + action_space.low) / 2.0).to(device)\n",
    "        print(self.action_scale, self.action_bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "        out = self.f(state)\n",
    "        mean = self.mean(out)\n",
    "        log_std = self.log_std(out)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sampling(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        x_t = dist.rsample()  # Reparameterization trick\n",
    "        squashed = torch.tanh(x_t)\n",
    "        log_prob = dist.log_prob(x_t)\n",
    "        log_prob -= torch.log(self.action_scale*(1 - squashed.pow(2) + 1e-4))\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        action = self.action_scale*squashed+ self.action_bias\n",
    "        return action, log_prob\n",
    "\n",
    "# Define Q Network\n",
    "class QDNN(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(QDNN, self).__init__()\n",
    "        layers = [LinearLayer(s_dim + a_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "        self.f = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        return self.f(x)\n",
    "\n",
    "# Define Critic\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.Q1 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "        self.Q2 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q1 = self.Q1(state, action)\n",
    "        q2 = self.Q2(state, action)\n",
    "        return q1, q2\n",
    "\n",
    "# Define Custom Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, s_dim, a_dim, buffer_capacity=1000000):\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.size = 0\n",
    "        self.ptr = 0\n",
    "\n",
    "        self.states = np.zeros((buffer_capacity, s_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((buffer_capacity, a_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((buffer_capacity, 1), dtype=np.float32)\n",
    "        self.next_states = np.zeros((buffer_capacity, s_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((buffer_capacity, 1), dtype=np.float32)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state.squeeze()\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.buffer_capacity\n",
    "        if self.size < self.buffer_capacity:\n",
    "            self.size += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        states = torch.FloatTensor(self.states[idxs])\n",
    "        actions = torch.FloatTensor(self.actions[idxs])\n",
    "        rewards = torch.FloatTensor(self.rewards[idxs])\n",
    "        next_states = torch.FloatTensor(self.next_states[idxs])\n",
    "        dones = torch.FloatTensor(self.dones[idxs])\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "# Define SAC\n",
    "class SAC:\n",
    "    def __init__(self, s_dim, a_dim, action_space, hidden_dim_actor=256, hidden_dim_critic=256, \n",
    "                 num_layer_actor=2, num_layer_critic=2, lr_act=3e-4, lr_crit=3e-4, \n",
    "                 gamma=0.99, tau=0.005, alpha=0.2, target_upd_inter=1, \n",
    "                 buffer_capacity=1000, batch_size=32, grad_steps = 1, device=\"cpu\"):\n",
    "        \n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.target_upd_inter = target_upd_inter\n",
    "        self.grad_steps = grad_steps\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.actor = ActorStochastic(s_dim, a_dim, action_space, hidden_dim_actor, num_hidden_layers=num_layer_actor, device=device).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_act)\n",
    "\n",
    "        self.critic = Critic(s_dim, a_dim, hidden_dim_critic, num_hidden_layers=num_layer_critic).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_crit)\n",
    "\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(device)\n",
    "        for p in self.critic_target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(s_dim, a_dim, buffer_capacity)\n",
    "\n",
    "    def choose_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        if evaluate:\n",
    "            with torch.no_grad():\n",
    "                action, _ = self.actor.sampling(state)\n",
    "        else:\n",
    "            action, _ = self.actor.sampling(state)\n",
    "\n",
    "        #print(action.shape, mean.shape)\n",
    "            \n",
    "        return action.cpu().detach().numpy()[0]\n",
    "\n",
    "    def add_to_buffer(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def train(self, update_interval):\n",
    "        if self.replay_buffer.size < self.batch_size:\n",
    "            return None, None\n",
    "\n",
    "        actor_l = 0\n",
    "        critic_l =0\n",
    "        for _ in range(self.grad_steps):\n",
    "            states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "            states, actions, rewards, next_states, dones = states.to(self.device), actions.to(self.device), rewards.to(self.device), next_states.to(self.device), dones.to(self.device)\n",
    "            #print(states.shape, actions.shape, rewards.shape, next_states.shape, dones.shape)\n",
    "            with torch.no_grad():\n",
    "                next_actions, next_log_probs = self.actor.sampling(next_states)\n",
    "                q1_next, q2_next = self.critic_target(next_states, next_actions)\n",
    "                min_q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs\n",
    "                q_target = rewards + (1 - dones) * self.gamma * min_q_next#.detach()\n",
    "                \n",
    "\n",
    "            q1, q2 = self.critic(states, actions)\n",
    "            critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            critic_l+=critic_loss.item()\n",
    "\n",
    "            actions_sample, log_probs = self.actor.sampling(states)\n",
    "            q1_actor, q2_actor = self.critic(states, actions_sample)\n",
    "            min_q_actor = torch.min(q1_actor, q2_actor)\n",
    "            actor_loss = (self.alpha * log_probs - min_q_actor.detach()).mean()\n",
    "\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            actor_l += actor_loss.item()\n",
    "\n",
    "            if update_interval% self.target_upd_inter==0:\n",
    "                for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "        return critic_loss.item(), actor_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the environment\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Hyperparameters\n",
    "# s_dim = env.observation_space.shape[0]\n",
    "# a_dim = env.action_space.shape[0]\n",
    "# action_space = env.action_space\n",
    "# print(action_space)\n",
    "# hidden_dim_actor = 64\n",
    "# hidden_dim_critic = 64\n",
    "# num_layer_actor = 1\n",
    "# num_layer_critic = 1\n",
    "# lr_act = 1e-4\n",
    "# lr_crit = 1e-4\n",
    "# gamma = 0.99\n",
    "# tau = 0.05\n",
    "# alpha = 0.2\n",
    "# batch_size = 64\n",
    "# num_episodes = 100\n",
    "# update_interval = 1\n",
    "# target_upd_inter = 1\n",
    "# buffer_capacity = 10000\n",
    "# num_warmup = 1000\n",
    "\n",
    "# # Initialize SAC agent\n",
    "# agent = SAC(s_dim, a_dim, action_space, hidden_dim_actor, hidden_dim_critic, \n",
    "#             num_layer_actor, num_layer_critic, lr_act, lr_crit, gamma, tau, \n",
    "#             alpha, buffer_capacity, batch_size, device)\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# episode_rewards = []\n",
    "# step_warmup = 0\n",
    "# for episode in range(num_episodes):\n",
    "#     state, _ = env.reset()\n",
    "#     episode_reward = 0\n",
    "#     for t in range(999):\n",
    "#         action = agent.choose_action(state, evaluate=True)\n",
    "#         next_state, reward, done, _, _ = env.step([action])\n",
    "#         agent.add_to_buffer(state, action, reward, next_state, done)\n",
    "#         state = next_state\n",
    "#         episode_reward += reward\n",
    "        \n",
    "#         if step_warmup>num_warmup:\n",
    "#             if t % update_interval == 0:\n",
    "#                 agent.update_parameters(update_interval)\n",
    "#         else:\n",
    "#             step_warmup+=1\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#     episode_rewards.append(episode_reward)\n",
    "#     if (episode + 1) % 5 == 0:\n",
    "#         print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
    "\n",
    "# # Plotting\n",
    "# plt.plot(episode_rewards)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Reward')\n",
    "# plt.title('SAC on MountainCarContinuous-v0')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the learning progress\n",
    "def plot_scores(score_history):\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Plot the scores with specified colors and labels\n",
    "    ax.plot(np.arange(1, len(score_history) + 1), score_history, color='green', label='SAC')\n",
    "\n",
    "    # Set the labels with a larger font size\n",
    "    ax.set_ylabel('Total reward (= time balanced)', fontsize=20)\n",
    "    ax.set_xlabel('Episode #', fontsize=20)\n",
    "\n",
    "    # Set the tick labels to a larger font size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "    # Add a legend with a specified font size\n",
    "    ax.legend(fontsize=20)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SAC(agent, env, n_games, training_interval=1, update_interval=1, print_num_episodes=10):\n",
    "    \n",
    "    n_games = n_games\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    training_interval=training_interval\n",
    "    update_interval = update_interval\n",
    "    warmup = 100\n",
    "\n",
    "    for i in range(n_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        for _ in range(999):\n",
    "            if warmup<n_steps:\n",
    "                action = agent.choose_action(state, evaluate=True)\n",
    "            else :\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            next_state, reward, done, _ = env.step([action])\n",
    "            score += reward\n",
    "            agent.add_to_buffer(state, action, reward, next_state, done)\n",
    "            state = next_state.squeeze()\n",
    "            update_interval += 1\n",
    "            n_steps += 1\n",
    "            if n_steps % training_interval == 0:\n",
    "                critic_loss, actor_loss = agent.train(update_interval)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-10:])\n",
    "        if i%print_num_episodes == 0:\n",
    "            print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n",
    "            print(critic_loss, actor_loss)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    return score_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain car continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0') tensor([0.], device='cuda:0')\n",
      "episode 0 score -40.3 avg score -40.3\n",
      "0.006383098661899567 -0.07244058698415756\n",
      "episode 1 score -40.2 avg score -40.3\n",
      "0.004982287529855967 -0.08368213474750519\n",
      "episode 2 score -41.3 avg score -40.6\n",
      "0.004791480954736471 -0.08633096516132355\n",
      "episode 3 score -42.3 avg score -41.1\n",
      "0.004109279252588749 -0.09202234447002411\n",
      "episode 4 score -38.6 avg score -40.6\n",
      "0.004047255031764507 -0.09919430315494537\n",
      "episode 5 score -40.6 avg score -40.6\n",
      "0.0036762775853276253 -0.10208681970834732\n",
      "episode 6 score -39.4 avg score -40.4\n",
      "0.003534425050020218 -0.10729130357503891\n",
      "episode 7 score -38.7 avg score -40.2\n",
      "0.0030260044150054455 -0.11255401372909546\n",
      "episode 8 score -38.7 avg score -40.0\n",
      "0.002824080642312765 -0.11815676093101501\n",
      "episode 9 score -39.8 avg score -40.0\n",
      "0.0026043406687676907 -0.1175815612077713\n",
      "episode 10 score -40.2 avg score -40.0\n",
      "0.0026553261559456587 -0.12480642646551132\n",
      "episode 11 score -39.8 avg score -40.0\n",
      "0.0023161815479397774 -0.12932613492012024\n",
      "episode 12 score -38.6 avg score -39.7\n",
      "0.0023416918702423573 -0.1366964727640152\n",
      "episode 13 score -36.8 avg score -39.1\n",
      "0.0023897369392216206 -0.1389085054397583\n",
      "episode 14 score -38.4 avg score -39.1\n",
      "0.0021505183540284634 -0.14467507600784302\n",
      "episode 15 score -36.5 avg score -38.7\n",
      "0.002163622295483947 -0.1471724808216095\n",
      "episode 16 score -36.6 avg score -38.4\n",
      "0.0021977934520691633 -0.15137231349945068\n",
      "episode 17 score -37.3 avg score -38.3\n",
      "0.0020221187733113766 -0.15580600500106812\n",
      "episode 18 score -38.4 avg score -38.2\n",
      "0.001902551855891943 -0.15929462015628815\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m a_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m SAC(s_dim, a_dim, env\u001b[38;5;241m.\u001b[39maction_space, buffer_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m5e5\u001b[39m), hidden_dim_actor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, hidden_dim_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m      6\u001b[0m             num_layer_actor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_layer_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lr_act\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-5\u001b[39m, lr_crit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-5\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, grad_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m score_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_SAC\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_num_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m plot_scores(score_history)\n",
      "Cell \u001b[1;32mIn[44], line 20\u001b[0m, in \u001b[0;36mtrain_SAC\u001b[1;34m(agent, env, n_games, training_interval, update_interval, print_num_episodes)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m999\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m warmup\u001b[38;5;241m<\u001b[39mn_steps:\n\u001b[1;32m---> 20\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[0;32m     22\u001b[0m         action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[1;32mIn[41], line 160\u001b[0m, in \u001b[0;36mSAC.choose_action\u001b[1;34m(self, state, evaluate)\u001b[0m\n\u001b[0;32m    156\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39msampling(state)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m#print(action.shape, mean.shape)\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "agent = SAC(s_dim, a_dim, env.action_space, buffer_capacity=int(5e5), hidden_dim_actor = 64, hidden_dim_critic=64, \n",
    "            num_layer_actor=1, num_layer_critic=1, lr_act=3e-5, lr_crit=3e-5, alpha=0.1,gamma=1,tau=0.01, batch_size=512, grad_steps=1, device=\"cuda\")\n",
    "\n",
    "score_history = train_SAC(agent, env, 200, training_interval=64, update_interval=128, print_num_episodes=1)\n",
    "plot_scores(score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0003\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], device='cuda:0') tensor([0.], device='cuda:0')\n",
      "episode 0 score -1217.8 avg score -1217.8\n",
      "777.2091674804688 67.92366790771484\n",
      "episode 1 score -923.4 avg score -1070.6\n",
      "2815.3828125 148.61643981933594\n",
      "episode 2 score -962.7 avg score -1034.6\n",
      "160.09140014648438 193.47061157226562\n",
      "episode 3 score -1468.3 avg score -1143.1\n",
      "155.70513916015625 246.41368103027344\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "agent = SAC(s_dim, a_dim, env.action_space,  buffer_capacity=int(1e6), hidden_dim_actor = 64, hidden_dim_critic=64, \n",
    "            num_layer_actor=1, num_layer_critic=1, lr_act=3e-4, lr_crit=5e-4, alpha=0.2, tau=0.05,grad_steps=2, batch_size=32, device=\"cuda\")\n",
    "\n",
    "score_history = train_SAC(agent, env, 200, training_interval=1, update_interval=1, print_num_episodes=1)\n",
    "plot_scores(score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
