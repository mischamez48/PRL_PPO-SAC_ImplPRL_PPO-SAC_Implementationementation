{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorStochastic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, action_boundaries, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(ActorStochastic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "        self.mean = nn.Linear(hidden_dim, a_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, a_dim)\n",
    "\n",
    "        self.action_boundaries = action_boundaries\n",
    "\n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        out = self.f(state)\n",
    "        mean = self.mean(out)\n",
    "        log_std = self.log_std(out)\n",
    "\n",
    "        #out = torch.tanh(out)\n",
    "        #log_std = torch.tanh(log_std)\n",
    "        #std = torch.exp(log_std)\n",
    "        #noise = torch.normal(0, 1, out.shape[0])\n",
    "        #action = mean + noise * std\n",
    "        #dist=torch.distributions.Normal(mean, log_std)\n",
    "        # log_std = torch.clamp(log_std, -20, 2)\n",
    "        \n",
    "        log_std = torch.clamp(log_std, -2, 2)\n",
    "        \n",
    "        return mean, log_std\n",
    "\n",
    "    def sampling(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        x_t = dist.rsample() #reparametrization trick implemented by pytorch\n",
    "        action = torch.tanh(x_t) #Bounds the action\n",
    "        log_prob = dist.log_prob(x_t) # Log probability(/ies if state in batch)\n",
    "        log_Jacobian = torch.log(1-action**2+1e-10).sum(dim=1, keepdim=True)\n",
    "        #print(log_Jacobian)\n",
    "        log_prob -=log_Jacobian\n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDNN(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(QDNN, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim + a_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        out = self.f(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        # Two DNNs to mitigate positive bias\n",
    "        self.Q1 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "        self.Q2 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q1 = self.Q1(state, action)\n",
    "        q2 = self.Q2(state, action)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC():\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim_actor=256, hidden_dim_critic=256, \n",
    "                 num_layer_actor=2, num_layer_critic=2, lr_act=3e-4, lr_crit=3e-4, \n",
    "                 gamma=0.99, tau=0.005, alpha=0.2, lambd=0.005, target_upd_inter=1, \n",
    "                 buffer_capacity=int(1000), batch_size=32, grad_steps = 1, device=\"cpu\"):\n",
    "        \n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=buffer_capacity)\n",
    "        self.grad_steps = grad_steps\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.target_upd_inter = target_upd_inter\n",
    "\n",
    "        self.actor = ActorStochastic(s_dim, a_dim, hidden_dim_actor, num_hidden_layers=num_layer_actor).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_act)\n",
    "\n",
    "        self.critic = Critic(s_dim, a_dim, hidden_dim_critic, num_hidden_layers=num_layer_critic).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_crit)\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(device)\n",
    "\n",
    "    def choose_action(self, state, evaluate=False):\n",
    "        #print(state.shape)\n",
    "        if evaluate:\n",
    "            # Choosing action to give to the environnement and not train the model\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                action, log_prob = self.actor.sampling(state)\n",
    "\n",
    "            return action.cpu().detach().numpy()[0], log_prob\n",
    "        \n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        # To train actor model and critic\n",
    "        action, log_prob = self.actor.sampling(state)\n",
    "        \n",
    "        return action, log_prob\n",
    "    \n",
    "    def critic_train(self, states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.choose_action(next_states, evaluate=False)\n",
    "            q1_next, q2_next = self.critic_target(next_states, next_actions)\n",
    "            min_q_next = torch.min(q1_next, q2_next)\n",
    "            target_q_value = rewards + self.gamma *(torch.ones_like(dones)-dones).unsqueeze(1)*(min_q_next - self.alpha * next_log_probs)\n",
    "     \n",
    "        \n",
    "        #print(target_q_value.shape)\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        #print(q1.shape, q2.shape)    \n",
    "        critic_loss = F.mse_loss(q1, target_q_value) + F.mse_loss(q2, target_q_value)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        return critic_loss\n",
    "\n",
    "    \n",
    "    def actor_train(self, states):\n",
    "        actions, log_probs = self.choose_action(states, evaluate=False)\n",
    "        q1_actor, q2_actor = self.critic(states, actions)\n",
    "        min_q_actor = torch.min(q1_actor, q2_actor)\n",
    "\n",
    "        #print(actions.shape, log_probs.shape)\n",
    "            \n",
    "        actor_loss = (self.alpha * log_probs.unsqueeze(1) - min_q_actor).mean()\n",
    "\n",
    "        #print(actor_loss)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return actor_loss\n",
    "\n",
    "    def train(self, update_interval):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None, None\n",
    "\n",
    "        for i in range(self.grad_steps):\n",
    "            \n",
    "            states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "            \n",
    "            # Critic train\n",
    "            self.critic.train()\n",
    "            critic_loss= self.critic_train(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            # Actor train\n",
    "            self.actor.train()\n",
    "            actor_loss = self.actor_train(states)\n",
    "\n",
    "            # Soft update of target networks\n",
    "            if update_interval % self.target_upd_inter == 0:\n",
    "                for target_parameters, parameters in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "                    target_parameters.data.copy_(self.tau * parameters.data + (1.0 - self.tau) * target_parameters.data)\n",
    "\n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "\n",
    "    def add_elements_to_buffer(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\AppData\\Local\\Temp\\ipykernel_3960\\1294076069.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic loss:  0.026462532579898834 actor loss :  -6.294869899749756\n",
      "critic loss:  0.01235608197748661 actor loss :  -8.125930786132812\n",
      "critic loss:  0.0032486505806446075 actor loss :  -8.753801345825195\n",
      "critic loss:  0.006489700637757778 actor loss :  -8.840635299682617\n",
      "critic loss:  0.8339219093322754 actor loss :  -8.201393127441406\n",
      "critic loss:  0.003921173978596926 actor loss :  -8.504669189453125\n",
      "critic loss:  0.003561805235221982 actor loss :  -8.508953094482422\n",
      "critic loss:  2.3079466819763184 actor loss :  -8.50600814819336\n",
      "critic loss:  0.023715583607554436 actor loss :  -8.06904125213623\n",
      "critic loss:  0.00984214711934328 actor loss :  -8.285770416259766\n",
      "Episode 10, Reward: -28.531345378714583\n",
      "critic loss:  1.856057047843933 actor loss :  -8.389808654785156\n",
      "critic loss:  0.008682656101882458 actor loss :  -8.44276237487793\n",
      "critic loss:  0.03514431044459343 actor loss :  -8.338154792785645\n",
      "critic loss:  0.006717576179653406 actor loss :  -8.245298385620117\n",
      "critic loss:  0.005361576564610004 actor loss :  -8.416900634765625\n",
      "critic loss:  0.009152944199740887 actor loss :  -8.774581909179688\n",
      "critic loss:  0.0054866899736225605 actor loss :  -8.623130798339844\n",
      "critic loss:  0.004327354487031698 actor loss :  -8.534055709838867\n",
      "critic loss:  0.010572921484708786 actor loss :  -8.419831275939941\n",
      "critic loss:  0.003224825719371438 actor loss :  -8.500992774963379\n",
      "Episode 20, Reward: -28.313879387517296\n",
      "critic loss:  1.9376451969146729 actor loss :  -8.573716163635254\n",
      "critic loss:  0.005403823219239712 actor loss :  -8.540918350219727\n",
      "critic loss:  0.013443714007735252 actor loss :  -8.196150779724121\n",
      "critic loss:  0.010534257628023624 actor loss :  -7.734714508056641\n",
      "critic loss:  0.015010051429271698 actor loss :  -7.5417399406433105\n",
      "critic loss:  1.442917823791504 actor loss :  -7.870643615722656\n",
      "critic loss:  0.005800395272672176 actor loss :  -7.98685359954834\n",
      "critic loss:  0.005040933843702078 actor loss :  -7.983858585357666\n",
      "critic loss:  0.003782124724239111 actor loss :  -8.31885051727295\n",
      "critic loss:  0.006520724855363369 actor loss :  -8.583813667297363\n",
      "Episode 30, Reward: -28.716572507148232\n",
      "critic loss:  1.696103572845459 actor loss :  -8.106718063354492\n",
      "critic loss:  0.005250396206974983 actor loss :  -8.254585266113281\n",
      "critic loss:  0.00664437934756279 actor loss :  -8.169686317443848\n",
      "critic loss:  0.008339357562363148 actor loss :  -8.232342720031738\n",
      "critic loss:  0.012459898367524147 actor loss :  -7.837759494781494\n",
      "critic loss:  0.004864118993282318 actor loss :  -8.046457290649414\n",
      "critic loss:  0.0038017211481928825 actor loss :  -8.118080139160156\n",
      "critic loss:  0.004844248294830322 actor loss :  -8.43056869506836\n",
      "critic loss:  0.005812949500977993 actor loss :  -7.875330448150635\n",
      "critic loss:  0.009202860295772552 actor loss :  -8.0354585647583\n",
      "Episode 40, Reward: -29.654473888902885\n",
      "critic loss:  0.010834204033017159 actor loss :  -7.870387554168701\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m update_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 50\u001b[0m     critic_loss, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 85\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, update_interval)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_steps):\n\u001b[1;32m---> 85\u001b[0m     states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Critic train\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[51], line 110\u001b[0m, in \u001b[0;36mSAC.sample_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(states, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    109\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 110\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_states, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    112\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(dones, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "hidden_dim_actor = 128\n",
    "hidden_dim_critic = 128\n",
    "num_layer_actor = 2\n",
    "num_layer_critic = 2\n",
    "lr_act = 3e-4\n",
    "lr_crit = 3e-4\n",
    "gamma = 0.99\n",
    "tau = 0.05\n",
    "alpha = 0.2\n",
    "batch_size = 64\n",
    "num_episodes = 100\n",
    "update_interval = 1\n",
    "target_upd_inter = 1\n",
    "buffer_capacity = int(1e3)\n",
    "grad_step=2\n",
    "\n",
    "#print(a_dim)\n",
    "# Initialize SAC agent\n",
    "agent = SAC(s_dim, a_dim, hidden_dim_actor=hidden_dim_actor, hidden_dim_critic=hidden_dim_critic, \n",
    "            num_layer_actor=num_layer_actor, num_layer_critic=num_layer_critic, lr_act=lr_act, \n",
    "            lr_crit=lr_crit, gamma=gamma, tau=tau, alpha=alpha, batch_size=batch_size, \n",
    "            grad_steps=grad_step, device=device, buffer_capacity=buffer_capacity)\n",
    "\n",
    "# Training loop\n",
    "episode_rewards = []\n",
    "critic_loss=None\n",
    "actor_loss=None\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() # shape of (2,)\n",
    "    episode_reward = 0\n",
    "    for t in range(1000):\n",
    "        #print(state.shape)\n",
    "        action, _ = agent.choose_action(state, evaluate=True)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.add_elements_to_buffer(state, action, reward, next_state, done)\n",
    "        state = next_state.copy()\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if t % update_interval == 0:\n",
    "            critic_loss, actor_loss = agent.train(t)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    if critic_loss is not None:\n",
    "        print(\"critic loss: \",critic_loss, \"actor loss : \", actor_loss)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
    "\n",
    "# Plotting\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('SAC on MountainCarContinuous-v0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5606734  0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done, _ = env.step(np.array([0]))\n",
    "#next_state, reward, done, _ = env.step([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(next_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "agent.add_elements_to_buffer(state, 0, reward, next_state, done)\n",
    "state = next_state\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.85067993], dtype=float32), tensor([[-1.7696]], device='cuda:0'))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.choose_action(state, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
