{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, a_dim))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "        out = self.f(state)\n",
    "        #out = torch.tanh(out)\n",
    "        out= F.softmax(out+1e-10, dim=-1)\n",
    "        return out\n",
    "    \n",
    "    def sampling(self, state):\n",
    "\n",
    "        probs = self(state)\n",
    "        dist = Categorical(probs)\n",
    "        sampled_actions = dist.sample()\n",
    "        log_probs = dist.log_prob(sampled_actions).unsqueeze(1)\n",
    "        return probs, log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDNN(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(QDNN, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        layers = [LinearLayer(s_dim + a_dim, hidden_dim)]\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(LinearLayer(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.f = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        out = self.f(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=256, num_hidden_layers=2):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        # Two DNNs to mitigate positive bias\n",
    "        self.Q1 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "        self.Q2 = QDNN(s_dim, a_dim, hidden_dim, num_hidden_layers)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q1 = self.Q1(state, action)\n",
    "        q2 = self.Q2(state, action)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC():\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim_actor=256, hidden_dim_critic=256, \n",
    "                 num_layer_actor=2, num_layer_critic=2, lr_act=3e-4, lr_crit=3e-4, \n",
    "                 gamma=0.99, tau=0.005, alpha=0.2, lambd=0.005, target_upd_inter=1, \n",
    "                 buffer_capacity=1000, batch_size=32, grad_steps = 1, device=\"cpu\"):\n",
    "        \n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=buffer_capacity)\n",
    "        self.grad_steps = grad_steps\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.target_upd_inter = target_upd_inter\n",
    "\n",
    "        self.actor = Actor(s_dim, a_dim, hidden_dim_actor, num_hidden_layers=num_layer_actor).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_act)\n",
    "\n",
    "        self.critic = Critic(s_dim, a_dim, hidden_dim_critic, num_hidden_layers=num_layer_critic).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_crit)\n",
    "\n",
    "        # Stabilize training\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(device)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        self.actor.eval()\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_probs = self.actor(state.to(self.device))\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "        return action.item(), action_probs.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    def train(self, update_interval):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None, None\n",
    "\n",
    "        for i in range(self.grad_steps):\n",
    "            batch = random.sample(self.buffer, self.batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).squeeze().to(self.device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "            #print(dones)\n",
    "            \n",
    "            # # Critic train\n",
    "            # self.critic.train()\n",
    "            \n",
    "            # with torch.no_grad():\n",
    "            #     probs = self.actor(next_states)\n",
    "            #     dist = Categorical(probs)\n",
    "            #     sampled_actions = dist.sample()\n",
    "            #     next_actions_log_probs = dist.log_prob(sampled_actions).unsqueeze(1)\n",
    "            #     q1_next, q2_next = self.critic_target(next_states, probs)\n",
    "            #     min_q_next = torch.min(q1_next, q2_next)\n",
    "            #     #print((torch.ones_like(dones)-dones).shape)\n",
    "            #     target_q_value = rewards + self.gamma *(torch.ones_like(dones)-dones).unsqueeze(1)*(min_q_next - self.alpha * next_actions_log_probs)#.sum(-1, keepdim=True))\n",
    "                \n",
    "            # q1, q2 = self.critic(states, actions)\n",
    "            \n",
    "            # critic_loss = F.mse_loss(q1, target_q_value.detach()) + F.mse_loss(q2, target_q_value.detach())\n",
    "\n",
    "            # self.critic_optimizer.zero_grad()\n",
    "            # critic_loss.backward()\n",
    "            # self.critic_optimizer.step()\n",
    "\n",
    "            # # Actor train\n",
    "            # self.actor.train()\n",
    "\n",
    "            # probs = self.actor(states)\n",
    "            # dist = Categorical(probs)\n",
    "            # sampled_actions = dist.sample()\n",
    "            # log_probs = dist.log_prob(sampled_actions)#.sum(-1, keepdim=True)\n",
    "            # q1_actor, q2_actor = self.critic(states, probs)\n",
    "            # min_q_actor = torch.min(q1_actor, q2_actor)\n",
    "            \n",
    "            # actor_loss = (self.alpha * log_probs.unsqueeze(1) - min_q_actor).mean()\n",
    "\n",
    "            # self.actor_optimizer.zero_grad()\n",
    "            # actor_loss.backward()\n",
    "            # self.actor_optimizer.step()\n",
    "\n",
    "            critic_loss = self.critic_train(states, actions, next_states, rewards, dones)\n",
    "\n",
    "            # Actor train\n",
    "            actor_loss = self.train_actor(states)\n",
    "\n",
    "            # Soft update of target networks\n",
    "            if update_interval % self.target_upd_inter == 0:\n",
    "                for target_parameters, parameters in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "                    target_parameters.data.copy_(self.tau * parameters.data + (1.0 - self.tau) * target_parameters.data)\n",
    "\n",
    "        #print(critic_loss.item(), actor_loss.item())\n",
    "        return critic_loss, actor_loss\n",
    "    \n",
    "    def critic_train(self, states, actions, next_states, rewards, dones):\n",
    "        with torch.no_grad():\n",
    "            probs, actions_log_probs = self.actor.sampling(states)\n",
    "            q1_next, q2_next = self.critic_target(next_states, probs)\n",
    "            min_q_next = torch.min(q1_next, q2_next)\n",
    "            #print((torch.ones_like(dones)-dones).shape)\n",
    "            target_q_value = rewards + self.gamma *(torch.ones_like(dones)-dones).unsqueeze(1)*(min_q_next - self.alpha * actions_log_probs)#.sum(-1, keepdim=True))\n",
    "                \n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        \n",
    "        critic_loss = F.mse_loss(q1, target_q_value.detach()) + F.mse_loss(q2, target_q_value.detach())\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        return critic_loss.item()\n",
    "    \n",
    "    def train_actor(self, states):\n",
    "        probs, log_probs = self.actor.sampling(states)\n",
    "        q1_actor, q2_actor = self.critic(states, probs)\n",
    "        min_q_actor = torch.min(q1_actor, q2_actor)\n",
    "        \n",
    "        actor_loss = (self.alpha * log_probs.unsqueeze(1) - min_q_actor).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return actor_loss.item()\n",
    "\n",
    "    def add_elements_to_buffer(self, state, probs, reward, next_state, done):\n",
    "        self.buffer.append((state, probs, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the learning progress\n",
    "def plot_scores(score_history):\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Plot the scores with specified colors and labels\n",
    "    ax.plot(np.arange(1, len(score_history) + 1), score_history, color='green', label='SAC')\n",
    "\n",
    "    # Set the labels with a larger font size\n",
    "    ax.set_ylabel('Total reward (= time balanced)', fontsize=20)\n",
    "    ax.set_xlabel('Episode #', fontsize=20)\n",
    "\n",
    "    # Set the tick labels to a larger font size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "    # Add a legend with a specified font size\n",
    "    ax.legend(fontsize=20)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SAC(agent, env, n_games, training_interval=1, update_interval=1, print_num_episodes=10):\n",
    "    env.seed(0)\n",
    "    n_games = n_games\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    training_interval=training_interval\n",
    "    update_interval = update_interval\n",
    "\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, probs = agent.choose_action(observation)\n",
    "            observation_, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            agent.add_elements_to_buffer(observation, probs, reward, observation_, done)\n",
    "            observation = observation_\n",
    "            update_interval += 1\n",
    "            n_steps += 1\n",
    "            if n_steps % training_interval == 0:\n",
    "                critic_loss, actor_loss = agent.train(update_interval)\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        if i%print_num_episodes == 0:\n",
    "            print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n",
    "            #print(critic_loss, actor_loss, value_loss)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    return score_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\registration.py:592: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 score 19.0 avg score 19.0\n",
      "episode 5 score 10.0 avg score 16.7\n",
      "episode 10 score 10.0 avg score 13.6\n",
      "episode 15 score 8.0 avg score 12.2\n",
      "episode 20 score 9.0 avg score 12.6\n",
      "episode 25 score 9.0 avg score 11.8\n",
      "episode 30 score 8.0 avg score 11.3\n",
      "episode 35 score 33.0 avg score 15.4\n",
      "episode 40 score 64.0 avg score 20.0\n",
      "episode 45 score 59.0 avg score 23.0\n",
      "episode 50 score 200.0 avg score 29.9\n",
      "episode 55 score 200.0 avg score 44.0\n",
      "episode 60 score 200.0 avg score 56.5\n",
      "episode 65 score 200.0 avg score 66.7\n",
      "episode 70 score 200.0 avg score 75.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m SAC(state_dim, action_dim, buffer_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e6\u001b[39m), hidden_dim_actor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, hidden_dim_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m      6\u001b[0m             num_layer_actor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_layer_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, lr_act\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6e-4\u001b[39m, lr_crit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6e-4\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, \n\u001b[0;32m      7\u001b[0m             grad_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 9\u001b[0m score_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_SAC\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_num_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m plot_scores(score_history)\n",
      "Cell \u001b[1;32mIn[45], line 27\u001b[0m, in \u001b[0;36mtrain_SAC\u001b[1;34m(agent, env, n_games, training_interval, update_interval, print_num_episodes)\u001b[0m\n\u001b[0;32m     25\u001b[0m     n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_steps \u001b[38;5;241m%\u001b[39m training_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         critic_loss, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m score_history\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m     30\u001b[0m avg_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(score_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:])\n",
      "Cell \u001b[1;32mIn[42], line 90\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, update_interval)\u001b[0m\n\u001b[0;32m     50\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(dones, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#print(dones)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# # Critic train\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# actor_loss.backward()\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# self.actor_optimizer.step()\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Actor train\u001b[39;00m\n\u001b[0;32m     93\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_actor(states)\n",
      "Cell \u001b[1;32mIn[42], line 106\u001b[0m, in \u001b[0;36mSAC.critic_train\u001b[1;34m(self, states, actions, next_states, rewards, dones)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    105\u001b[0m     probs, actions_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39msampling(states)\n\u001b[1;32m--> 106\u001b[0m     q1_next, q2_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     min_q_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(q1_next, q2_next)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m#print((torch.ones_like(dones)-dones).shape)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 13\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[1;32m---> 13\u001b[0m     q1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ2(state, action)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q1, q2\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 16\u001b[0m, in \u001b[0;36mQDNN.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[1;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(x)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "agent = SAC(state_dim, action_dim, buffer_capacity=int(1e6), hidden_dim_actor = 64, hidden_dim_critic=64, \n",
    "            num_layer_actor=2, num_layer_critic=2, lr_act=6e-4, lr_crit=6e-4, alpha=0.2, tau=0.05, batch_size=20, \n",
    "            grad_steps=2, device=device)\n",
    "\n",
    "score_history = train_SAC(agent, env, 200, training_interval=1, update_interval=1, print_num_episodes=5)\n",
    "plot_scores(score_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 score 31.0 avg score 31.0\n",
      "episode 5 score 9.0 avg score 16.7\n",
      "episode 10 score 9.0 avg score 13.2\n",
      "episode 15 score 10.0 avg score 12.1\n",
      "episode 20 score 10.0 avg score 13.0\n",
      "episode 25 score 46.0 avg score 15.8\n",
      "episode 30 score 123.0 avg score 26.9\n",
      "episode 35 score 325.0 avg score 51.5\n",
      "episode 40 score 276.0 avg score 94.6\n",
      "episode 45 score 497.0 avg score 130.5\n",
      "episode 50 score 380.0 avg score 160.1\n",
      "episode 55 score 292.0 avg score 179.6\n",
      "episode 60 score 500.0 avg score 205.0\n",
      "episode 65 score 500.0 avg score 226.4\n",
      "episode 70 score 500.0 avg score 245.7\n",
      "episode 75 score 500.0 avg score 260.5\n",
      "episode 80 score 500.0 avg score 275.3\n",
      "episode 85 score 500.0 avg score 288.3\n",
      "episode 90 score 500.0 avg score 300.0\n",
      "episode 95 score 500.0 avg score 310.4\n",
      "episode 100 score 500.0 avg score 320.3\n",
      "episode 105 score 500.0 avg score 344.6\n",
      "episode 110 score 500.0 avg score 369.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m SAC(state_dim, action_dim, buffer_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e6\u001b[39m), hidden_dim_actor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, hidden_dim_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m      6\u001b[0m             num_layer_actor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_layer_critic\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lr_act\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6e-4\u001b[39m, lr_crit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6e-4\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[0;32m      7\u001b[0m             grad_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 9\u001b[0m score_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_SAC\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_num_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m plot_scores(score_history)\n",
      "Cell \u001b[1;32mIn[45], line 27\u001b[0m, in \u001b[0;36mtrain_SAC\u001b[1;34m(agent, env, n_games, training_interval, update_interval, print_num_episodes)\u001b[0m\n\u001b[0;32m     25\u001b[0m     n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_steps \u001b[38;5;241m%\u001b[39m training_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         critic_loss, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m score_history\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m     30\u001b[0m avg_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(score_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:])\n",
      "Cell \u001b[1;32mIn[42], line 98\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, update_interval)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_interval \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_upd_inter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m target_parameters, parameters \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[1;32m---> 98\u001b[0m             \u001b[43mtarget_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m#print(critic_loss.item(), actor_loss.item())\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m critic_loss, actor_loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "agent = SAC(state_dim, action_dim, buffer_capacity=int(1e6), hidden_dim_actor = 64, hidden_dim_critic=64, \n",
    "            num_layer_actor=1, num_layer_critic=1, lr_act=6e-4, lr_crit=6e-4, alpha=0.2, tau=0.05, batch_size=32, \n",
    "            grad_steps=2, device=device)\n",
    "\n",
    "score_history = train_SAC(agent, env, 250, training_interval=1, update_interval=1, print_num_episodes=5)\n",
    "plot_scores(score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
